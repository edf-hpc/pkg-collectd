#! /bin/sh /usr/share/dpatch/dpatch-run
## slurmd-plugin.dpatch by Rémi Palancher <remi@rezib.org>
##
## DP: plugin: slurmd plugin

@DPATCH@
diff -urNad '--exclude=CVS' '--exclude=.svn' '--exclude=.git' '--exclude=.arch' '--exclude=.hg' '--exclude=_darcs' '--exclude=.bzr' git_edf-hpc_pkg-collectd~/configure.ac git_edf-hpc_pkg-collectd/configure.ac
--- git_edf-hpc_pkg-collectd~/configure.ac	2018-03-15 14:38:47.000000000 +0100
+++ git_edf-hpc_pkg-collectd/configure.ac	2018-03-15 15:18:52.000000000 +0100
@@ -5209,6 +5209,7 @@
 AC_PLUGIN([sensors],     [$with_libsensors],   [lm_sensors statistics])
 AC_PLUGIN([serial],      [$plugin_serial],     [serial port traffic])
 AC_PLUGIN([sigrok],      [$with_libsigrok],    [sigrok acquisition sources])
+AC_PLUGIN([slurmd],      [yes],                [Slurmd jobs statistics])
 AC_PLUGIN([snmp],        [$with_libnetsnmp],   [SNMP querying plugin])
 AC_PLUGIN([statsd],      [yes],                [StatsD plugin])
 AC_PLUGIN([swap],        [$plugin_swap],       [Swap usage statistics])
@@ -5553,6 +5554,7 @@
     sensors . . . . . . . $enable_sensors
     serial  . . . . . . . $enable_serial
     sigrok  . . . . . . . $enable_sigrok
+    slurmd  . . . . . . . $enable_slurmd
     snmp  . . . . . . . . $enable_snmp
     statsd  . . . . . . . $enable_statsd
     swap  . . . . . . . . $enable_swap
diff -urNad '--exclude=CVS' '--exclude=.svn' '--exclude=.git' '--exclude=.arch' '--exclude=.hg' '--exclude=_darcs' '--exclude=.bzr' git_edf-hpc_pkg-collectd~/src/Makefile.am git_edf-hpc_pkg-collectd/src/Makefile.am
--- git_edf-hpc_pkg-collectd~/src/Makefile.am	2018-03-15 14:38:47.000000000 +0100
+++ git_edf-hpc_pkg-collectd/src/Makefile.am	2018-03-15 15:18:52.000000000 +0100
@@ -1072,6 +1072,14 @@
 collectd_DEPENDENCIES += sigrok.la
 endif
 
+if BUILD_PLUGIN_SLURMD
+pkglib_LTLIBRARIES += slurmd.la
+slurmd_la_SOURCES = slurmd.c
+slurmd_la_LDFLAGS = -module -avoid-version
+collectd_LDADD += "-dlopen" slurmd.la
+collectd_DEPENDENCIES += slurmd.la
+endif
+
 if BUILD_PLUGIN_SNMP
 pkglib_LTLIBRARIES += snmp.la
 snmp_la_SOURCES = snmp.c
diff -urNad '--exclude=CVS' '--exclude=.svn' '--exclude=.git' '--exclude=.arch' '--exclude=.hg' '--exclude=_darcs' '--exclude=.bzr' git_edf-hpc_pkg-collectd~/src/collectd.conf.in git_edf-hpc_pkg-collectd/src/collectd.conf.in
--- git_edf-hpc_pkg-collectd~/src/collectd.conf.in	2018-03-15 14:38:47.000000000 +0100
+++ git_edf-hpc_pkg-collectd/src/collectd.conf.in	2018-03-15 15:18:52.000000000 +0100
@@ -156,6 +156,7 @@
 #@BUILD_PLUGIN_SENSORS_TRUE@LoadPlugin sensors
 #@BUILD_PLUGIN_SERIAL_TRUE@LoadPlugin serial
 #@BUILD_PLUGIN_SIGROK_TRUE@LoadPlugin sigrok
+#@BUILD_PLUGIN_SLURMD_TRUE@LoadPlugin slurmd
 #@BUILD_PLUGIN_SNMP_TRUE@LoadPlugin snmp
 #@BUILD_PLUGIN_STATSD_TRUE@LoadPlugin statsd
 #@BUILD_PLUGIN_SWAP_TRUE@LoadPlugin swap
@@ -917,6 +918,11 @@
 #  </Device>
 #</Plugin>
 
+#<Plugin slurmd>
+#  CgroupMountPoint "/sys/fs/cgroup"
+#  IgnoreAbsentCpuset false
+#</Plugin>
+
 #<Plugin snmp>
 #   <Data "powerplus_voltge_input">
 #       Type "voltage"
diff -urNad '--exclude=CVS' '--exclude=.svn' '--exclude=.git' '--exclude=.arch' '--exclude=.hg' '--exclude=_darcs' '--exclude=.bzr' git_edf-hpc_pkg-collectd~/src/collectd.conf.pod git_edf-hpc_pkg-collectd/src/collectd.conf.pod
--- git_edf-hpc_pkg-collectd~/src/collectd.conf.pod	2018-03-15 14:38:47.000000000 +0100
+++ git_edf-hpc_pkg-collectd/src/collectd.conf.pod	2018-03-15 15:50:14.062347576 +0100
@@ -5260,6 +5260,72 @@
 
 =back
 
+=head2 Plugin C<slurmd>
+
+The I<slurmd plugin> collects Slurm HPC workload manager jobs ressource usage
+information (CPU time, memory consumption) on compute nodes.
+
+B<Synopsis:>
+
+  <Plugin slurmd>
+      CgroupMountPoint "/sys/fs/cgroup"
+      IgnoreAbsentCpuset false
+  </Plugin>
+
+=over 4
+
+=item B<CgroupMountPoint> I<MountPoint>
+
+Browse the I<MountPoint> directory to find Cgroup resource controllers
+filesystems. These Cgroup filesystems are then used to find running PIDs inside
+Slurm jobs and discover which CPU are allocated to jobs.
+
+=item B<IgnoreAbsentCpuset> B<true>|B<false>
+
+When set to B<false>, the plugin returns an error if the cpuset Cgroup
+hierarchy maintained by Slurm is not found. When set to B<true>, the plugin
+ignores the problem, it just does not submit any metric without error.
+
+The Slurm Cgroup hierarchies are created on first job execution only.
+Therefore, it is recommanded to set this parameter to B<true> to avoid the
+plugin returning errors and then being increasingly delayed on compute node
+boot.
+
+=item B<SlurmdNodeName> I<NodeName>
+
+Slurm will create cgroups under the directory I<cpuset/slurm> in
+B<CgroupMountPoint> by default. When Slurm is built with support for multiple
+Slurmd by node, it will use a path including the nodename declared for the
+node: I<cpuset/slurm_NODENAME>. This plugin will try both and will by default
+use the hostname returned by C<gethostname()>. If this does not fit the current
+nodename used by slurmd, this option permits to give the true nodename to use
+to the plugin.
+
+=item B<CollectPSS> B<true>|B<false>
+
+The plugin can collect memory usage per job by summing the PSS (Proportional
+Set Size) of each process associated to the job on the current node. This metric
+is the most precise for memory usage but collecting it incurs a performance
+penalty that might not be neglected. PSS is the sum of all the PSS entries in
+I</proc/PID/smaps> for each process.
+
+When B<false>, the metric is emitted with a value of 0.
+
+Default: B<true>
+
+=item B<CollectRSS> B<true>|B<false>
+
+The plugin can collect memory usage per job by summing the RSS (Resident
+Set Size) of each process associated to the job on the current node. This metric
+can double count some memory usage on shared memory but is the fastest to collect.
+This is the sum of all RSS entries in I</proc/PID/stat> for each process.
+
+When B<false>, the metric is emitted with a value of 0.
+
+Default: B<true>
+
+=back
+
 =head2 Plugin C<snmp>
 
 Since the configuration of the C<snmp plugin> is a little more complicated than
diff -urNad '--exclude=CVS' '--exclude=.svn' '--exclude=.git' '--exclude=.arch' '--exclude=.hg' '--exclude=_darcs' '--exclude=.bzr' git_edf-hpc_pkg-collectd~/src/slurmd.c git_edf-hpc_pkg-collectd/src/slurmd.c
--- git_edf-hpc_pkg-collectd~/src/slurmd.c	1970-01-01 01:00:00.000000000 +0100
+++ git_edf-hpc_pkg-collectd/src/slurmd.c	2018-03-15 15:18:52.000000000 +0100
@@ -0,0 +1,1115 @@
+/**
+ * collectd - src/slurmd.c
+ * Copyright (C) 2015       Rémi Palancher
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the
+ * Free Software Foundation; only version 2 of the License is applicable.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+ *
+ * Authors:
+ *   Rémi Palancher <remi at rezib.org>
+ **/
+
+#include "collectd.h"
+#include "common.h"
+#include "plugin.h"
+#include "configfile.h"
+
+#define MAX_JOBS 1024
+#define MAX_JOB_NB_SIZE 16
+#define MAX_JOB_CPUS 1024
+#define MAX_CPUS_STR_SIZE 128
+#define MAX_CPU_NAME_SIZE 128
+#define MAX_PIDS 1024
+
+static const char *config_keys[] =
+{
+  "CgroupMountPoint",
+  "IgnoreAbsentCpuset",
+  "SlurmdNodeName",
+  "CollectPSS",
+  "CollectRSS"
+};
+
+static int config_keys_num = STATIC_ARRAY_SIZE (config_keys);
+
+/*
+ * Contains a snapshot view of consumed CPU time.
+ */
+
+struct cpu_time_s
+{
+  long unsigned int user;
+  long unsigned int nice;
+  long unsigned int system;
+  long unsigned int idle;
+  long unsigned int iowait;
+  long unsigned int irq;
+  long unsigned int softirq;
+  long unsigned int steal;
+  long unsigned int guest;
+  long unsigned int guest_nice;
+};
+
+typedef struct cpu_time_s cpu_time_t;
+
+/*
+ * Contains a snapshot view of consumed memory.
+ */
+
+struct memory_s
+{
+  long unsigned int rss;
+  long unsigned int pss;
+};
+
+typedef struct memory_s memory_t;
+
+/*
+ * Contains all informations about a job, with:
+ *   - job number
+ *   - global PSS
+ *   - CPU usage counters
+ */
+
+typedef struct job_info_s job_info_t;
+
+struct job_info_s {
+  unsigned int job_number;
+  cpu_time_t *cpu_time;
+  memory_t *memory;
+  _Bool updated;
+  job_info_t *next;
+};
+
+typedef struct job_info_s jobs_list_t;
+
+/* configuration parameters */
+static char *cgroup_mnt_pt = NULL;
+static char *slurmd_node_name = NULL;
+static _Bool ignore_absent_cpuset = 0;
+static _Bool collect_pss = 1;
+static _Bool collect_rss = 1;
+/* global vars */
+static jobs_list_t *jobs = NULL;
+
+/*
+ * jobs_cpu_time_list manipulation utilities
+ */
+
+/*
+ * Adds a new job to the global jobs list.
+ */
+static job_info_t * slurmd_job_add (unsigned int job_number, cpu_time_t *initial_cpu_time, memory_t *initial_memory)
+{
+  job_info_t *new_job = smalloc(sizeof(job_info_t));
+  job_info_t *xjob = NULL;
+
+  bzero(new_job, sizeof(job_info_t));
+
+  new_job->job_number = job_number;
+  new_job->cpu_time = initial_cpu_time;
+  new_job->memory = initial_memory;
+
+  new_job->updated = 1;
+  new_job->next = NULL;
+
+  if (jobs == NULL) {
+    jobs = new_job;
+  } else {
+    xjob = jobs;
+    while (xjob->next)
+      xjob = xjob->next;
+    xjob->next = new_job;
+  }
+
+  return new_job;
+} /* job_info_t * slurmd_job_add */
+
+/*
+ * Update the cpu time of the job with the new_cpu_time in parameter.
+ */
+static void slurmd_job_update_cpu_time (job_info_t *job, cpu_time_t *new_cpu_time)
+{
+  sfree(job->cpu_time);
+  job->cpu_time = new_cpu_time;
+  job->updated = 1;
+} /* void slurmd_job_update_cpu_time */
+
+/*
+ * Update the memory of the job with the memory in parameter.
+ */
+static void slurmd_job_update_memory (job_info_t *job, memory_t *new_memory)
+{
+  sfree(job->memory);
+  job->memory = new_memory;
+  job->updated = 1;
+} /* void slurmd_job_update_memory */
+
+/*
+ * Finds the job whose number is in parameter out of the global jobs list
+ * and returns a pointer to it. Return NULL if not found.
+ */
+static job_info_t * slurmd_job_find (unsigned int job_number)
+{
+  job_info_t *xjob = jobs;
+
+  while(xjob) {
+    if(xjob->job_number == job_number) {
+      return xjob;
+    }
+    xjob = xjob->next;
+  }
+  return NULL;
+} /* job_info_t * slurmd_job_find */
+
+/*
+ * Flags all jobs as outdated in the jobs global list.
+ */
+static void slurmd_jobs_flag_outdated (void)
+{
+  job_info_t *xjob = jobs;
+
+  while(xjob) {
+    xjob->updated = 0;
+    xjob = xjob->next;
+  }
+} /* void slurmd_jobs_flag_outdated */
+
+/*
+ * Removes all jobs flagged as outdated in the jobs global list.
+ */
+static void slurmd_jobs_remove_outdated (void)
+{
+  job_info_t *cur_job, *next_job;
+
+  if (jobs == NULL)
+    return;
+
+  cur_job = jobs;
+  next_job = cur_job->next;
+
+  /* if first elements of jobs list are outdated, remove them */
+  while(cur_job && !cur_job->updated) {
+    sfree(cur_job->cpu_time);
+    sfree(cur_job);
+    jobs = cur_job = next_job;
+    if (next_job)
+      next_job = next_job->next;
+  }
+
+  /* then iterate until the end of the list to remove outdated jobs */
+  while(next_job) {
+    if(!next_job->updated) {
+      cur_job->next = next_job->next;
+      sfree(next_job->cpu_time);
+      sfree(next_job);
+    } else {
+      /* jump to next job */
+      cur_job = next_job;
+    }
+    next_job = cur_job->next;
+  }
+} /* void slurmd_jobs_remove_outdated */
+
+static void slurmd_submit_value (long unsigned int job_number, char *type, char *type_instance, value_t value[])
+{
+  value_list_t vl = VALUE_LIST_INIT;
+
+  sstrncpy (vl.host, hostname_g, sizeof (vl.host));
+  sstrncpy (vl.plugin, "slurmd", sizeof (vl.plugin));
+  ssnprintf(vl.plugin_instance, sizeof(vl.plugin_instance), "job_%lu", job_number);
+  sstrncpy (vl.type, type, sizeof(vl.type));
+  sstrncpy (vl.type_instance, type_instance, sizeof(vl.type_instance));
+
+  vl.values = value;
+  vl.values_len = 1;
+
+  plugin_dispatch_values (&vl);
+} /* void slurmd_submit_value */
+
+static void slurmd_submit_gauge (long unsigned int job_number, char *type, char *type_instance, gauge_t value)
+{
+  value_t values[1];
+  values[0].gauge = value;
+  DEBUG("slurmd plugin: submitting gauge for job %lu %s: %f", job_number, type_instance, value);
+  slurmd_submit_value(job_number, type, type_instance, values);
+} /* void slurmd_submit_gauge */
+
+static void slurmd_submit_derive (long unsigned int job_number, char *type, char *type_instance, derive_t value)
+{
+  value_t values[1];
+  values[0].derive = value;
+  DEBUG("slurmd plugin: submitting derive for job %lu %s: %"PRIi64, job_number, type_instance, value);
+  slurmd_submit_value(job_number, type, type_instance, values);
+} /* void slurmd_submit_derive */
+
+/*
+ * Submit all jobs' metrics
+ */
+static void slurmd_jobs_report_metrics (void)
+{
+  job_info_t *xjob = jobs;
+
+  while(xjob) {
+
+    /* PSS is in kBytes, submit in Bytes */
+    slurmd_submit_gauge(xjob->job_number, "memory", "pss",
+                        (gauge_t) (xjob->memory->pss * 1024));
+    /* RSS already in Bytes */
+    slurmd_submit_gauge(xjob->job_number, "memory", "rss",
+                        (gauge_t) xjob->memory->rss);
+    slurmd_submit_derive(xjob->job_number, "cpu", "user",
+                         (derive_t) xjob->cpu_time->user);
+    slurmd_submit_derive(xjob->job_number, "cpu", "nice",
+                         (derive_t) xjob->cpu_time->nice);
+    slurmd_submit_derive(xjob->job_number, "cpu", "system",
+                         (derive_t) xjob->cpu_time->system);
+    slurmd_submit_derive(xjob->job_number, "cpu", "idle",
+                         (derive_t) xjob->cpu_time->idle);
+    slurmd_submit_derive(xjob->job_number, "cpu", "iowait",
+                         (derive_t) xjob->cpu_time->iowait);
+    slurmd_submit_derive(xjob->job_number, "cpu", "irq",
+                         (derive_t) xjob->cpu_time->irq);
+    slurmd_submit_derive(xjob->job_number, "cpu", "softirq",
+                         (derive_t) xjob->cpu_time->softirq);
+    slurmd_submit_derive(xjob->job_number, "cpu", "steal",
+                         (derive_t) xjob->cpu_time->steal);
+    slurmd_submit_derive(xjob->job_number, "cpu", "guest",
+                         (derive_t) xjob->cpu_time->guest);
+    slurmd_submit_derive(xjob->job_number, "cpu", "guest_nice",
+                         (derive_t) xjob->cpu_time->guest_nice);
+
+    xjob = xjob->next;
+
+  }
+} /* void slurmd_jobs_report_metrics */
+
+/*
+ * Does total += to_add for all cpu_time_t members.
+ */
+static void slurmd_add_cpu_time (cpu_time_t *total, cpu_time_t *to_add)
+{
+  total->user += to_add->user;
+  total->nice += to_add->nice;
+  total->system += to_add->system;
+  total->idle += to_add->idle;
+  total->iowait += to_add->iowait;
+  total->irq += to_add->irq;
+  total->softirq += to_add->softirq;
+  total->steal += to_add->steal;
+  total->guest += to_add->guest;
+  total->guest_nice += to_add->guest_nice;
+} /* void slurmd_add_cpu_time */
+
+#if COLLECT_DEBUG
+/*
+ * Fill the string str in parameter with a string representation of all values
+ * in the cpu_time_t in parameter.
+ */
+void slurmd_snprintf_cpu_times (char *str, int strlen, cpu_time_t *times)
+{
+  ssnprintf(str, strlen,
+            "user: %lu nice: %lu system: %lu idle: %lu iowait: %lu irq: %lu "
+            "softirq: %lu steal: %lu guest: %lu guest_nice: %lu",
+            times->user,
+            times->nice,
+            times->system,
+            times->idle,
+            times->iowait,
+            times->irq,
+            times->softirq,
+            times->steal,
+            times->guest,
+            times->guest_nice);
+} /* void slurmd_snprintf_cpu_times */
+#endif
+
+/*
+ * Returns an array of char* with all tokens extracted for a_str splitted
+ * with a_delim.
+ */
+static char ** slurmd_str_split (char* a_str, const char a_delim)
+{
+  char **result = 0;
+  size_t count = 0;
+  char *tmp = a_str;
+  char *last_delim = 0;
+  char delim[2];
+  char *save_ptr;
+  delim[0] = a_delim;
+  delim[1] = 0;
+
+  /* Count how many elements will be extracted. */
+  while (*tmp) {
+    if (a_delim == *tmp) {
+      count++;
+      last_delim = tmp;
+    }
+    tmp++;
+  }
+
+  /* Add space for trailing token. */
+  count += last_delim < (a_str + strlen(a_str) - 1);
+
+  /* Add space for terminating null string so caller
+     knows where the list of returned strings ends. */
+  count++;
+
+  result = smalloc(sizeof(char*) * count);
+
+  if (result) {
+    size_t idx  = 0;
+    char* token = strtok_r(a_str, delim, &save_ptr);
+
+    while (token) {
+      assert(idx < count);
+      *(result + idx++) = sstrdup(token);
+      token = strtok_r(NULL, delim, &save_ptr);
+    }
+    assert(idx == count - 1);
+    *(result + idx) = 0;
+  }
+
+  return result;
+} /* char ** slurmd_str_split */
+
+/*
+ * Convert a string to a long and returns it, or -1 on error.
+ */
+static inline long slurmd_str_to_long (const char *cpu_str)
+{
+  char *endptr;
+  long val = 0;
+  errno = 0;
+  val = strtol(cpu_str, &endptr, 10);
+
+  if ((errno == ERANGE && (val == LONG_MAX || val == LONG_MIN))
+       || (errno != 0 && val == 0)) {
+    char errbuf[1024];
+    ERROR("slurmd plugin: error with strtol(): %s", sstrerror (errno, errbuf, sizeof (errbuf)));
+    return (-1);
+  }
+
+  if (endptr == cpu_str) {
+    ERROR("slurmd plugin: no digits were found during strtol() conversion");
+    return (-1);
+  }
+
+  return val;
+} /* long slurmd_str_to_long */
+
+/*
+ * Sums all the PSS in /proc/<pid>/smaps for the PID given in parameter.
+ *
+ * Returns the sum of PSS of the PID or -1 on error.
+ */
+static unsigned int slurmd_get_pid_pss (const pid_t pid)
+{
+  char smaps_fpath[PATH_MAX];
+  char buf[1024];
+  FILE *smaps_fh;
+  int nb_matches;
+  long unsigned int pss = 0, xpss = 0;
+
+  /* build smaps fpath */
+  bzero(smaps_fpath, sizeof(smaps_fpath));
+  ssnprintf(smaps_fpath, sizeof(smaps_fpath), "/proc/%d/smaps", pid);
+
+  smaps_fh = fopen(smaps_fpath, "r");
+
+  if (smaps_fh == NULL) {
+    char errbuf[1024];
+    WARNING("slurmd plugin: fopen() error on smaps file %s: %s", smaps_fpath, sstrerror (errno, errbuf, sizeof (errbuf)));
+    return (-1);
+  }
+
+  /* Read all lines to make the sum of all Pss */
+  while (fgets(buf, sizeof(buf), smaps_fh)) {
+    nb_matches = sscanf(buf, "Pss: %lu kB\n", &xpss);
+    if (nb_matches == 1)
+      pss += xpss;
+  }
+
+  fclose(smaps_fh);
+  DEBUG("slurmd plugin: total PSS %lu kB for PID %i", pss, pid);
+  return pss;
+} /* int slurmd_get_pid_pss */
+
+/*
+ * VmRSS from /proc/<pid>/stat for the PID given in parameter.
+ *
+ * Returns the value of RSS of the PID or -1 on error.
+ */
+static unsigned int slurmd_get_pid_rss (const pid_t pid)
+{
+  char stat_fpath[PATH_MAX];
+  char buf[1024];
+  FILE *stat_fh;
+  int nb_matches;
+  long int rss_pages = 0;
+  long unsigned int rss = 0;
+
+  /* build stat fpath */
+  bzero(stat_fpath, sizeof(stat_fpath));
+  ssnprintf(stat_fpath, sizeof(stat_fpath), "/proc/%d/stat", pid);
+
+  stat_fh = fopen(stat_fpath, "r");
+
+  if (stat_fh == NULL) {
+    char errbuf[1024];
+    WARNING("slurmd plugin: fopen() error on stat file %s: %s", stat_fpath, sstrerror (errno, errbuf, sizeof (errbuf)));
+    return (-1);
+  }
+
+  /* Read all lines to make the sum of all Pss */
+  if (fgets(buf, sizeof(buf), stat_fh)) {
+    nb_matches = sscanf(buf, "%*d %*s %*c %*d %*d %*d %*d %*d %*u %*u %*u %*u %*u %*u %*u %*d %*d %*d %*d %*d %*d %*u %*u %ld ", &rss_pages);
+    if (nb_matches == 1)
+      rss = rss_pages * getpagesize();
+  }
+
+  fclose(stat_fh);
+  DEBUG("slurmd plugin: total RSS %lu kB for PID %i", rss, pid);
+  return rss;
+} /* int slurmd_get_pid_rss */
+
+/*
+ * Parses /proc/stat file calculature the sum of cpu time of all cpus
+ * allocated to a job and given in parameters.
+ *
+ * Returns 0 on success, -1 on error.
+ */
+static int slurmd_get_job_cpus_time (int job_cpus[], int job_nb_cpus, cpu_time_t *result)
+{
+  char cur_cpu_name[MAX_CPU_NAME_SIZE];
+  char searched_cpu_name[MAX_CPU_NAME_SIZE];
+  int cpu_idx = 0;
+  FILE *fstat = NULL;
+  _Bool all_cpus_found = 0;
+  cpu_time_t *xcpu_time = smalloc(sizeof(cpu_time_t));
+
+  bzero(searched_cpu_name, sizeof(searched_cpu_name));
+  bzero(cur_cpu_name, sizeof(cur_cpu_name));
+
+  /* initialize result to 0 before doing sums */
+  bzero(result, sizeof(cpu_time_t));
+  bzero(xcpu_time, sizeof(cpu_time_t));
+
+  fstat = fopen("/proc/stat", "r");
+
+  if (fstat == NULL) {
+    char errbuf[1024];
+    WARNING("slurmd plugin: fopen() error on file /proc/stat: %s", sstrerror (errno, errbuf, sizeof (errbuf)));
+    fclose(fstat);
+    return (-1);
+  }
+
+  ssnprintf(searched_cpu_name, MAX_CPU_NAME_SIZE, "cpu%d", job_cpus[cpu_idx]);
+  while (!all_cpus_found &&
+         (fscanf(fstat, "%s %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu\n",
+                 cur_cpu_name, &xcpu_time->user, &xcpu_time->nice,
+                 &xcpu_time->system, &xcpu_time->idle, &xcpu_time->iowait,
+                 &xcpu_time->irq, &xcpu_time->softirq, &xcpu_time->steal,
+                 &xcpu_time->guest, &xcpu_time->guest_nice)) != EOF) {
+    /*
+     * Check if CPU name matches.
+     */
+    if (strncmp(cur_cpu_name, searched_cpu_name, strlen(searched_cpu_name)) == 0) {
+#if COLLECT_DEBUG
+      char cpu_times_s[256];
+      slurmd_snprintf_cpu_times(cpu_times_s, sizeof(cpu_times_s), xcpu_time);
+      DEBUG("slurmd plugin: read %s jiffies: %s", cur_cpu_name, cpu_times_s);
+#endif
+      slurmd_add_cpu_time(result, xcpu_time);
+      cpu_idx++;
+      if (cpu_idx >= job_nb_cpus)
+        all_cpus_found = 1;
+      ssnprintf(searched_cpu_name, MAX_CPU_NAME_SIZE, "cpu%d", job_cpus[cpu_idx]);
+    }
+  }
+  fclose(fstat);
+  sfree(xcpu_time);
+
+  return (0);
+} /* int slurmd_get_job_cpus_time */
+
+/*
+ * Read the cpuset tasks file whose path is given in parameter and fill the
+ * pids array with all PIDs found in this file.
+ *
+ * Returns the number of found PIDs, -1 on error.
+ */
+static int slurmd_read_tasks_pids (const char *tasks_fpath, pid_t pids[])
+{
+  int xpid = 0;
+  int idx = 0;
+  FILE *ftasks = fopen(tasks_fpath, "r");
+
+  if (ftasks == NULL) {
+    char errbuf[1024];
+    WARNING("slurmd plugin: fopen() error on tasks file %s: %s", tasks_fpath, sstrerror (errno, errbuf, sizeof (errbuf)));
+    return (-1);
+  }
+
+  while (fscanf(ftasks, "%d\n", &xpid) != EOF)
+    pids[idx++] = xpid;
+
+  fclose(ftasks);
+  return idx;
+} /* int slurmd_read_tasks_pids */
+
+/*
+ * Returns 1 if pid is in threads array, or 0 if not.
+ */
+static inline _Bool slurmd_pid_is_thread (pid_t pid, pid_t threads[], int nb_threads)
+{
+  int i;
+
+  for (i=0; i<nb_threads; i++)
+    if (pid == threads[i])
+      return (1);
+
+  return (0);
+} /* _Bool slurmd_pid_is_thread */
+
+/*
+ * Spots threads among the PIDs in parameter by checking the content of the
+ * /proc/<pid>/task/ directory. Fill the threads array in parameter with the
+ * PIDs identified as threads.
+ *
+ * Returns the number of threads found.
+ */
+static int slurmd_track_threads (pid_t pids[], int nb_pids, pid_t threads[])
+{
+  int nb_threads = 0;
+  char task_dir_fpath[PATH_MAX];
+  DIR *dir;
+  struct dirent dir_ent_buf, *dir_ent;
+  int i;
+  pid_t pid, xtid;
+
+  /* initialize null-string */
+  bzero(task_dir_fpath, sizeof(task_dir_fpath));
+
+  for(i=0; i<nb_pids; i++) {
+    pid = pids[i];
+    if(!slurmd_pid_is_thread (pid, threads, nb_threads)) {
+      ssnprintf(task_dir_fpath, PATH_MAX, "/proc/%d/task", pid);
+
+      dir = opendir(task_dir_fpath);
+
+      /*
+       * If dir is NULL, it means the /proc/<pid>/task directory does not exist
+       * and that the PID itself is a thread so simply ignore this case.
+       */
+      if (dir != NULL) {
+        while(1) {
+          if (readdir_r(dir, &dir_ent_buf, &dir_ent) != 0) {
+            WARNING("slurmd plugin: problem during readdir_r() %s", task_dir_fpath);
+            continue;
+          }
+          if (dir_ent == NULL)
+            break;
+
+          /* exclude . and .. dir entries */
+          if ( strncmp(dir_ent->d_name, ".", 1) &&
+               strncmp(dir_ent->d_name, "..", 2) ) {
+            xtid = (pid_t)slurmd_str_to_long(dir_ent->d_name);
+            if (xtid == -1)
+               WARNING("slurmd plugin: weird task TID found %s for PID %d", dir_ent->d_name, pid);
+            else if (xtid != pid)
+               threads[nb_threads++] = xtid;
+          }
+        }
+        closedir(dir);
+      }
+    }
+  }
+
+  return nb_threads;
+} /* int slurmd_track_threads */
+
+/*
+ * Parses the PIDs found in the cpuset cgroup tasks file whose path is given
+ * in parameter and compute the memory usage of all these PIDs (excepting
+ * threads).
+ *
+ * Returns 0 on success or -1 on error.
+ */
+static int slurmd_get_tasks_memory (const char *tasks_fpath, memory_t* result)
+{
+  int idx = 0;
+  int nb_pids = -1;
+  int nb_threads = -1;
+  long unsigned int xpss, xrss;
+
+  pid_t pids[MAX_PIDS];
+  /* used to track threads and exclude them from memory accounting */
+  pid_t threads[MAX_PIDS];
+
+  bzero(pids, sizeof(pids));
+
+  nb_pids = slurmd_read_tasks_pids(tasks_fpath, pids);
+  if(nb_pids < 0)
+    return(-1);
+
+  nb_threads = slurmd_track_threads(pids, nb_pids, threads);
+  if(nb_pids < 0)
+    return (-1);
+
+  for(idx = 0 ; idx<nb_pids ; idx++) {
+    /* Exclude threads */
+    if (!slurmd_pid_is_thread(pids[idx], threads, nb_threads)) {
+      if (collect_pss) {
+        xpss = slurmd_get_pid_pss(pids[idx]);
+        if (xpss < 0)
+          return (-1);
+        else
+          result->pss += xpss;
+      }
+      else {
+        result->pss = 0;
+      }
+
+      if (collect_rss) {
+        xrss = slurmd_get_pid_rss(pids[idx]);
+        if (xrss < 0)
+          return (-1);
+        else
+          result->rss += xrss;
+      }
+      else {
+        result->rss = 0;
+      }
+    }
+  }
+
+  return 0;
+} /* int slurmd_get_tasks_memory */
+
+/*
+ * Browse a Slurm job step specific cgroup whose path is given in parameter
+ * to find the tasks file and get the sum of all theses tasks PSS.
+ *
+ * Returns 0 on success or -1 on error
+ */
+static int slurmd_get_jobstep_memory (const char *step_cpuset_mnt_pt, memory_t *result)
+{
+  char tasks_fpath[PATH_MAX];
+  DIR *dir;
+  struct dirent dir_ent_buf, *dir_ent;
+  memory_t * xresult=NULL;
+  int status = -1;
+
+  dir = opendir(step_cpuset_mnt_pt);
+  if (dir == NULL) {
+    WARNING("slurmd plugin: directory %s could not be open", step_cpuset_mnt_pt);
+    return (-1);
+  }
+
+  xresult = smalloc(sizeof(memory_t));
+  while(1) {
+    if (readdir_r(dir, &dir_ent_buf, &dir_ent) != 0) {
+      WARNING("slurmd plugin: problem during readdir_r() %s", step_cpuset_mnt_pt);
+      closedir(dir);
+      return (-1);
+    }
+    if (dir_ent == NULL)
+      break;
+
+    /* check if ent name starts with 'job_' */
+    if (strncmp(dir_ent->d_name, "tasks", 6) == 0) {
+      ssnprintf(tasks_fpath, sizeof(tasks_fpath), "%s/%s",
+                step_cpuset_mnt_pt, dir_ent->d_name);
+      xresult->rss = 0;
+      xresult->pss = 0;
+      status = slurmd_get_tasks_memory(tasks_fpath, xresult);
+      if (status < 0) {
+        closedir(dir);
+        return -1;
+      }
+      else {
+        result->pss += xresult->pss;
+        result->rss += xresult->rss;
+      }
+    }
+  }
+  closedir(dir);
+
+  return 0;
+} /* int slurmd_get_jobstep_memory */
+
+/*
+ * Parses cpuset cpuset.cpus file of the cpuset cgroup whose path is given in
+ * parameter, then it fills cpus array with the list of CPUs numbers found in
+ * this file.
+ *
+ * Returns the number of CPUs or -1 on error.
+ */
+static int slurmd_parse_cpuset_cpus_list (const char* cpuset_fpath, int cpus[])
+{
+  char cpus_fpath[PATH_MAX];
+  char **tokens;
+  char *token, *inter_end, *save_ptr;
+  char cpus_str[MAX_CPUS_STR_SIZE];
+  FILE *cpus_fh;
+  int i;
+  int cpu_idx = 0;
+  int xcpu_id, xcpu_id_start, xcpu_id_end;
+
+  bzero(cpus_fpath, sizeof(cpus_fpath));
+  bzero(cpus_str, sizeof(cpus_str));
+
+  ssnprintf(cpus_fpath, sizeof(cpus_fpath), "%s/%s", cpuset_fpath, "cpuset.cpus");
+  cpus_fh = fopen(cpus_fpath, "r");
+
+  if (cpus_fh == NULL) {
+    char errbuf[1024];
+    WARNING("slurmd plugin: cpuset.cpus fopen() error on file %s: %s", cpus_fpath, sstrerror (errno, errbuf, sizeof (errbuf)));
+    return (-1);
+  }
+
+  if(fscanf(cpus_fh, "%s\n", cpus_str) < 1) {
+    char errbuf[1024];
+    WARNING("slurmd plugin: fscanf() error on file %s: %s", cpus_fpath, sstrerror (errno, errbuf, sizeof (errbuf)));
+    fclose(cpus_fh);
+    return (-1);
+  }
+  fclose(cpus_fh);
+
+  /*
+   * Ex. file content: 1-4,6
+   * 1st split on commas, then for each element check if interval and compute
+   * it if so.
+   */
+  tokens = slurmd_str_split(cpus_str, ',');
+  for (i = 0; (token = *(tokens + i)) != NULL; i++) {
+
+    inter_end = strtok_r(token, "-", &save_ptr);
+    inter_end = strtok_r(NULL, "-", &save_ptr);
+
+    if (inter_end == NULL) {
+      /* It is just a CPU number, not an interval of CPUs.
+       * It just has to be converted into integer.
+       */
+      xcpu_id = slurmd_str_to_long(token);
+      if (xcpu_id != -1) {
+        DEBUG("slurmd plugin: adding CPU number [%d] %d", cpu_idx, xcpu_id);
+        cpus[cpu_idx++] = xcpu_id;
+      }
+      else
+        WARNING("slurmd plugin: weird CPU number %s found in cpuset.cpus file %s", token, cpus_fpath);
+    } else {
+
+      /* This a CPUs interval */
+      xcpu_id_start = slurmd_str_to_long(token);
+      xcpu_id_end = slurmd_str_to_long(inter_end);
+      if(xcpu_id_start != -1 && xcpu_id_end != -1)
+        for (xcpu_id = xcpu_id_start; xcpu_id <= xcpu_id_end; xcpu_id++) {
+          DEBUG("slurmd plugin: adding CPU number [%d] %d", cpu_idx, xcpu_id);
+          cpus[cpu_idx++] = xcpu_id;
+        }
+      else
+        WARNING("slurmd plugin: weird CPU number %s/%s found in cpuset.cpus file %s", token, inter_end, cpus_fpath);
+    }
+    sfree(token);
+
+  }
+  sfree(tokens);
+
+  return cpu_idx;
+} /* int slurmd_parse_cpuset_cpus_list */
+
+/*
+ * Browse Slurm Job cgroup cpuset hierarchy.
+ *
+ * First, it looks at the CPUs allocated to the job and get a new snapshot of
+ * those CPUs jiffies.
+ *
+ * Then, it browses all Slurm job's steps specific sub-cpusets to get all job's
+ * PIDs and measure their memory consumption.
+ *
+ * Returns 0 on success, -1 on error.
+ */
+static int slurmd_browse_job_cpuset (char* job_cpuset_mnt_pt, unsigned long int job_number)
+{
+  char step_cpuset_mnt_pt[PATH_MAX];
+  DIR *dir;
+  struct dirent dir_ent_buf, *dir_ent;
+  int job_cpus[MAX_JOB_CPUS];
+  int job_nb_cpus = 0;
+  int ret = 0;
+  int status = -1;
+  cpu_time_t *cpu_time = NULL;
+  memory_t *memory = NULL, *xmemory = NULL;
+  job_info_t *my_job;
+
+  /*
+   * Get job's allocated CPUs
+   */
+  job_nb_cpus = slurmd_parse_cpuset_cpus_list(job_cpuset_mnt_pt, job_cpus);
+
+  if(job_nb_cpus < 0)
+    return (-1);
+
+  /*
+   * Initial memory info
+   */
+  memory = smalloc(sizeof(memory_t));
+  memory->pss = 0;
+  memory->rss = 0;
+
+  /*
+   * Get job's allocated CPUs usage
+   */
+  cpu_time = smalloc(sizeof(cpu_time_t));
+  ret = slurmd_get_job_cpus_time(job_cpus, job_nb_cpus, cpu_time);
+
+  if (ret < 0)
+    return (-1);
+
+  my_job = slurmd_job_find(job_number);
+  if (my_job == NULL) {
+    my_job = slurmd_job_add(job_number, cpu_time, memory);
+  }
+  else {
+    slurmd_job_update_cpu_time(my_job, cpu_time);
+    slurmd_job_update_memory(my_job, memory);
+  }
+  /*
+   * Get all steps tasks usage
+   */
+  dir = opendir(job_cpuset_mnt_pt);
+  if (dir == NULL) {
+    WARNING("slurmd plugin: directory %s could not be open", job_cpuset_mnt_pt);
+    return (-1);
+  }
+
+  ret = 0;
+
+  xmemory = smalloc(sizeof(memory_t));
+  while(1) {
+    if (readdir_r(dir, &dir_ent_buf, &dir_ent) != 0) {
+      WARNING("slurmd plugin: problem during readdir_r() %s", job_cpuset_mnt_pt);
+      closedir(dir);
+      return (-1);
+    }
+    if (dir_ent == NULL)
+      break;
+
+    if (strncmp(dir_ent->d_name, "step_", 5) == 0) {
+        ssnprintf(step_cpuset_mnt_pt, sizeof(step_cpuset_mnt_pt), "%s/%s",
+                  job_cpuset_mnt_pt, dir_ent->d_name);
+        xmemory->pss = 0;
+        xmemory->rss = 0;
+        status = slurmd_get_jobstep_memory(step_cpuset_mnt_pt, xmemory);
+        if(status < 0)
+          ret = -1;
+        else
+          memory->pss += xmemory->pss;
+          memory->rss += xmemory->rss;
+    }
+  }
+  closedir(dir);
+
+  return ret;
+} /* int slurmd_browse_job_cpuset */
+
+/*
+ * Browse Slurm UID cgroup cpuset hierarchy to search for Slurm job specific
+ * sub-cpusets, and browse all of them.
+ *
+ * Returns 0 on success, -1 on error.
+ */
+static int slurmd_browse_uid_cpuset (const char *uid_cpuset_mnt_pt)
+{
+  char job_cpuset_mnt_pt[PATH_MAX];
+  DIR *dir;
+  struct dirent dir_ent_buf, *dir_ent;
+  char job_number_str[MAX_JOB_NB_SIZE];
+  char *job_number_addr = NULL;
+  unsigned long int job_number = 0;
+  int ret = 0; // return code
+
+  bzero(job_number_str, MAX_JOB_NB_SIZE);
+
+  dir = opendir(uid_cpuset_mnt_pt);
+  if (dir == NULL) {
+    ERROR("slurmd plugin: directory %s could not be open", uid_cpuset_mnt_pt);
+    return (-1);
+  }
+
+  /* At this point, return error code if ALL jobs' cpuset fail. If at least one
+     succeeds, return 0.
+  */
+  ret = -1;
+
+  while(1) {
+    if (readdir_r(dir, &dir_ent_buf, &dir_ent) != 0) {
+      WARNING("slurmd plugin: problem during readdir_r() %s", uid_cpuset_mnt_pt);
+      closedir(dir);
+      return (-1);
+    }
+    if (dir_ent == NULL)
+      break;
+
+    /* check if dir_ent name starts with 'job_' */
+    if (strncmp(dir_ent->d_name, "job_", 4) == 0) {
+
+      /*
+       * Extract job number out of dir entry name
+       */
+      job_number_addr = dir_ent->d_name + 4*sizeof(char);
+      strncpy(job_number_str, job_number_addr, strlen(job_number_addr));
+      job_number = slurmd_str_to_long(job_number_str);
+      if (job_number != -1) {
+        ssnprintf(job_cpuset_mnt_pt, sizeof(job_cpuset_mnt_pt), "%s/%s",
+                  uid_cpuset_mnt_pt, dir_ent->d_name);
+        if(!slurmd_browse_job_cpuset(job_cpuset_mnt_pt, job_number))
+          ret = 0;
+      } else
+        WARNING("slurmd plugin: weird job number %s found", job_number_str);
+    }
+  }
+  closedir(dir);
+  return ret;
+} /* int slurmd_browse_uid_cpuset */
+
+
+/*
+ * Browse Slurm cgroup cpuset hierarchy to search for Slurm UID specific
+ * sub-cpusets, and browse all of them.
+ *
+ * Returns 0 on success, -1 on error.
+ */
+static int slurmd_update_jobs_usage (void)
+{
+  char slurm_cpuset_mnt_pt[PATH_MAX];
+  char uid_cpuset_mnt_pt[PATH_MAX];
+  char node_name[HOST_NAME_MAX];
+  DIR *dir;
+  struct dirent dir_ent_buf, *dir_ent;
+  int ret = 0;
+
+  /* Try to open cpuset/slurm in cgroups */
+  bzero(slurm_cpuset_mnt_pt, PATH_MAX);
+  ssnprintf(slurm_cpuset_mnt_pt, sizeof(slurm_cpuset_mnt_pt), "%s/%s",
+            cgroup_mnt_pt, "cpuset/slurm");
+
+  dir = opendir(slurm_cpuset_mnt_pt);
+
+  /* There is no cpuset/slurm directory, check cpuset/slurm_<nodename> */
+  if (dir == NULL) {
+    bzero(node_name, HOST_NAME_MAX);
+    if (slurmd_node_name != NULL)
+    {
+      sstrncpy(node_name, slurmd_node_name, sizeof(slurmd_node_name));
+    } else {
+      if ( gethostname(node_name, HOST_NAME_MAX) != 0) {
+        ERROR("slurmd plugin: can't gethostname and none provided in config: %d", errno);
+        return(-1);
+      }
+    }
+
+    bzero(slurm_cpuset_mnt_pt, PATH_MAX);
+    ssnprintf(slurm_cpuset_mnt_pt, sizeof(slurm_cpuset_mnt_pt), "%s/%s_%s",
+              cgroup_mnt_pt, "cpuset/slurm", node_name);
+    dir = opendir(slurm_cpuset_mnt_pt);
+  }
+
+  if (dir == NULL) {
+    ERROR("slurmd plugin: directory %s could not be open", slurm_cpuset_mnt_pt);
+    /* if IgnoreAbsentCpuset conf parameter is true, ignore the error */
+    if (ignore_absent_cpuset)
+      return (0);
+    else
+      return (-1);
+  }
+
+  while(1) {
+    if (readdir_r(dir, &dir_ent_buf, &dir_ent) != 0) {
+      WARNING("slurmd plugin: problem during readdir_r() %s", slurm_cpuset_mnt_pt);
+      closedir(dir);
+      return (-1);
+    }
+    if (dir_ent == NULL)
+      break;
+
+    /* check if dir_ent name starts with 'uid_' */
+    if (strncmp(dir_ent->d_name, "uid_", 4) == 0) {
+      ssnprintf(uid_cpuset_mnt_pt, sizeof(uid_cpuset_mnt_pt), "%s/%s",
+                slurm_cpuset_mnt_pt, dir_ent->d_name);
+      if(slurmd_browse_uid_cpuset(uid_cpuset_mnt_pt))
+        ret = -1;
+    }
+  }
+  closedir(dir);
+  return ret;
+} /* int slurmd_update_jobs_usage */
+
+/* slurmd plugin read callback */
+static int slurmd_read (void)
+{
+  int update_status = 0;
+
+  /*
+   * Flag all job metrics as outdated, read new job metrics then remove all
+   * jobs still flagged as outdated.
+   */
+  slurmd_jobs_flag_outdated();
+  update_status = slurmd_update_jobs_usage();
+  slurmd_jobs_remove_outdated();
+
+  /* report metrics only if update_status is OK */
+  if(!update_status)
+      slurmd_jobs_report_metrics();
+
+  return update_status;
+} /* int slurmd_read */
+
+/* slurmd plugin config callback */
+static int slurmd_config (const char *key, const char *value)
+{
+  if (strcasecmp(key, "CgroupMountPoint") == 0) {
+    if (cgroup_mnt_pt != NULL)
+      sfree(cgroup_mnt_pt);
+    cgroup_mnt_pt = strdup(value);
+  } else if (0 == strcasecmp (key, "IgnoreAbsentCpuset")) {
+    if (IS_FALSE (value))
+      ignore_absent_cpuset = 0;
+    else
+      ignore_absent_cpuset = 1;
+  } else if (0 == strcasecmp (key, "SlurmdNodeName")) {
+    if (slurmd_node_name != NULL)
+      sfree(slurmd_node_name);
+    slurmd_node_name = strdup(value);
+  } else if (0 == strcasecmp (key, "CollectPSS")) {
+    if (IS_FALSE (value))
+      collect_pss = 0;
+    else
+      collect_pss = 1;
+  } else if (0 == strcasecmp (key, "CollectRSS")) {
+    if (IS_FALSE (value))
+      collect_rss = 0;
+    else
+      collect_rss = 1;
+  } else
+    return (-1);
+  return (0);
+} /* int slurmd_config */
+
+void module_register (void)
+{
+  plugin_register_config("slurmd", slurmd_config, config_keys, config_keys_num);
+  plugin_register_read("slurmd", slurmd_read);
+} /* void module_register */
